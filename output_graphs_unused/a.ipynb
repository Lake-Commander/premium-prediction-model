{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9a92da",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and file handling\n",
    "import os\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Advanced model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Saving models\n",
    "import joblib\n",
    "\n",
    "# Date\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aae6d7",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"Insurance Premium Prediction Dataset.csv\")\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n",
    "df[\"Annual Income\"].fillna(df[\"Annual Income\"].median(), inplace=True)\n",
    "df[\"Number of Dependents\"].fillna(df[\"Number of Dependents\"].median(), inplace=True)\n",
    "df[\"Credit Score\"].fillna(df[\"Credit Score\"].median(), inplace=True)\n",
    "df[\"Health Score\"].fillna(df[\"Health Score\"].median(), inplace=True)\n",
    "df[\"Premium Amount\"].fillna(df[\"Premium Amount\"].median(), inplace=True)\n",
    "\n",
    "# Fill missing categorical values\n",
    "df[\"Occupation\"].fillna(\"Unknown\", inplace=True)\n",
    "df[\"Customer Feedback\"].fillna(df[\"Customer Feedback\"].mode()[0], inplace=True)\n",
    "df[\"Marital Status\"].fillna(df[\"Marital Status\"].mode()[0], inplace=True)\n",
    "\n",
    "# Fill Previous Claims (assumed as no claims)\n",
    "df[\"Previous Claims\"].fillna(0.0, inplace=True)\n",
    "\n",
    "# Save cleaned dataset for preview\n",
    "df.to_csv(\"cleaned_insurance_data.csv\", index=False)\n",
    "\n",
    "# Preview result\n",
    "print(\"✅ Dataset cleaned successfully.\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Remaining missing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f82681",
   "metadata": {},
   "source": [
    "# Fixed data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ad86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"cleaned_insurance_data.csv\")  # Save file from previous session\n",
    "\n",
    "# Fix data types\n",
    "df[\"Number of Dependents\"] = df[\"Number of Dependents\"].astype(int)\n",
    "df[\"Previous Claims\"] = df[\"Previous Claims\"].astype(int)\n",
    "df[\"Policy Start Date\"] = pd.to_datetime(df[\"Policy Start Date\"], errors=\"coerce\")\n",
    "\n",
    "# Print results for confirmation\n",
    "print(\"✅ Data types corrected successfully.\")\n",
    "print(\"\\nUpdated Data Types:\\n\", df.dtypes)\n",
    "\n",
    "# Save the corrected dataset (optional)\n",
    "df.to_csv(\"Typed_Insurance_Dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c4744",
   "metadata": {},
   "source": [
    "# Address skewed distributions for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71391a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"Typed_Insurance_Dataset.csv\")\n",
    "\n",
    "# List of numerical columns to check for skew\n",
    "numeric_cols = [\n",
    "    \"Annual Income\", \"Credit Score\", \"Premium Amount\",\n",
    "    \"Health Score\", \"Age\", \"Number of Dependents\", \"Previous Claims\"\n",
    "]\n",
    "\n",
    "# Calculate skewness\n",
    "skew_vals = df[numeric_cols].skew()\n",
    "\n",
    "print(\"🔍 Skewness of numerical features:\")\n",
    "print(skew_vals)\n",
    "print(\"\\n📌 Highly skewed features (|skew| > 1):\")\n",
    "\n",
    "# Threshold for high skew\n",
    "threshold = 1\n",
    "high_skew_cols = skew_vals[abs(skew_vals) > threshold].index.tolist()\n",
    "print(high_skew_cols)\n",
    "\n",
    "# Apply log1p transformation to fix right skew\n",
    "for col in high_skew_cols:\n",
    "    # Skip columns with negative or NaN values\n",
    "    if (df[col] < 0).any():\n",
    "        print(f\"⚠️ Skipping {col} due to negative values.\")\n",
    "        continue\n",
    "    df[col + \"_log\"] = np.log1p(df[col])\n",
    "    print(f\"✅ Transformed {col} → {col}_log\")\n",
    "\n",
    "# visualize one transformed feature to preview\n",
    "if high_skew_cols:\n",
    "    first = high_skew_cols[0]\n",
    "    sns.histplot(df[first], kde=True)\n",
    "    plt.title(f\"{first} - Before Log Transform\")\n",
    "    plt.show()\n",
    "\n",
    "    sns.histplot(df[first + \"_log\"], kde=True)\n",
    "    plt.title(f\"{first} - After Log Transform\")\n",
    "    plt.show()\n",
    "\n",
    "# Save to new CSV for reference\n",
    "df.to_csv(\"transformed.csv\", index=False)\n",
    "print(\"\\n✅ Skewed features transformed and saved to 'cleaned_skew_fixed.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0dd5bc",
   "metadata": {},
   "source": [
    "# On to EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d16c64",
   "metadata": {},
   "source": [
    "# First step involved performing Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"transformed.csv\")  # From previous cell\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create directory for saving plots\n",
    "output_dir = \"plots/univariate\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------- Numerical Features -----------\n",
    "numerical_features = [\"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "                      \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\",\n",
    "                      \"Premium Amount\"]\n",
    "\n",
    "# Summary statistics\n",
    "print(\"🔢 Summary Statistics:\")\n",
    "print(df[numerical_features].describe())\n",
    "\n",
    "# Histograms for numerical features\n",
    "for col in numerical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[col], kde=True, color=\"skyblue\")\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    safe_name = col.lower().replace(\" \", \"_\")\n",
    "    plt.savefig(f\"{output_dir}/hist_{safe_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ----------- Categorical Features -----------\n",
    "categorical_features = [\"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "                        \"Location\", \"Policy Type\", \"Customer Feedback\", \"Smoking Status\",\n",
    "                        \"Exercise Frequency\", \"Property Type\"]\n",
    "\n",
    "# Count plots for categorical features\n",
    "for col in categorical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=df, x=col, palette=\"Set2\", order=df[col].value_counts().index)\n",
    "    plt.title(f\"Count Plot of {col}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    safe_name = col.lower().replace(\" \", \"_\")\n",
    "    plt.savefig(f\"{output_dir}/count_{safe_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✅ Univariate plots saved in: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0a73a",
   "metadata": {},
   "source": [
    "# Second step involved performing Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c34b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed.csv\")\n",
    "\n",
    "# ========== 1. NUMERICAL vs NUMERICAL ==========\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = df[[\"Annual Income\", \"Health Score\", \"Credit Score\", \"Premium Amount\"]].corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/bivariate_correlation_heatmap.png\")\n",
    "plt.close()\n",
    "\n",
    "# Scatterplot: Income vs Premium\n",
    "sns.scatterplot(data=df, x=\"Annual Income\", y=\"Premium Amount\", alpha=0.4)\n",
    "plt.title(\"Annual Income vs Premium Amount\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/scatter_income_premium.png\")\n",
    "plt.close()\n",
    "\n",
    "# ========== 2. CATEGORICAL vs NUMERICAL ==========\n",
    "cat_num_pairs = [\n",
    "    (\"Gender\", \"Premium Amount\"),\n",
    "    (\"Education Level\", \"Premium Amount\"),\n",
    "    (\"Occupation\", \"Premium Amount\"),\n",
    "    (\"Smoking Status\", \"Premium Amount\"),\n",
    "]\n",
    "\n",
    "for cat, num in cat_num_pairs:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df, x=cat, y=num, palette=\"Set3\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"{cat} vs {num}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/box_{cat.lower().replace(' ', '_')}_vs_{num.lower().replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ========== 3. CATEGORICAL vs CATEGORICAL ==========\n",
    "cat_cat_pairs = [\n",
    "    (\"Gender\", \"Smoking Status\"),\n",
    "    (\"Marital Status\", \"Exercise Frequency\"),\n",
    "    (\"Occupation\", \"Property Type\"),\n",
    "]\n",
    "\n",
    "for cat1, cat2 in cat_cat_pairs:\n",
    "    cross_tab = pd.crosstab(df[cat1], df[cat2])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(cross_tab, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "    plt.title(f\"{cat1} vs {cat2}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/heatmap_{cat1.lower().replace(' ', '_')}_vs_{cat2.lower().replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"✅ Bivariate analysis completed and plots saved to 'plots/' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5ed7a",
   "metadata": {},
   "source": [
    "# Third step involved performing Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"transformed.csv\")\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"plots/multivariate\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Numerical columns for correlation\n",
    "numerical_features = [\"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "                      \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\",\n",
    "                      \"Premium Amount\"]\n",
    "\n",
    "# ----------- 1. Correlation Heatmap -----------\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap (Numerical Features)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/correlation_heatmap.png\")\n",
    "plt.close()\n",
    "\n",
    "# ----------- 2. Pairplot (Sampled for speed) -----------\n",
    "sample_df = df[numerical_features].sample(2000, random_state=42)  # Reduce for speed/memory\n",
    "sns.pairplot(sample_df)\n",
    "plt.savefig(f\"{output_dir}/pairplot_numeric.png\")\n",
    "plt.close()\n",
    "\n",
    "# ----------- 3. Box plots vs. Premium Amount for selected categorical -----------\n",
    "categorical_features = [\"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "                        \"Policy Type\", \"Smoking Status\", \"Exercise Frequency\", \"Property Type\"]\n",
    "\n",
    "for cat in categorical_features:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(data=df, x=cat, y=\"Premium Amount\", palette=\"pastel\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Premium Amount by {cat}\")\n",
    "    plt.tight_layout()\n",
    "    safe_name = cat.lower().replace(\" \", \"_\")\n",
    "    plt.savefig(f\"{output_dir}/premium_by_{safe_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # ----------- 4. Grouped Bar Plots (Categorical vs Categorical with Hue) -----------\n",
    "\n",
    "# Grouped barplot examples\n",
    "grouped_combinations = [\n",
    "    (\"Smoking Status\", \"Exercise Frequency\", \"Policy Type\"),\n",
    "    (\"Education Level\", \"Marital Status\", \"Gender\"),\n",
    "    (\"Location\", \"Property Type\", \"Policy Type\"),\n",
    "]\n",
    "\n",
    "for x_col, y_col, hue_col in grouped_combinations:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=x_col, hue=hue_col, order=df[x_col].value_counts().index, palette=\"Set2\")\n",
    "    plt.title(f\"{x_col} vs {y_col} grouped by {hue_col}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    safe_name = f\"{x_col}_{y_col}_by_{hue_col}\".lower().replace(\" \", \"_\")\n",
    "    plt.savefig(f\"{output_dir}/groupedbar_{safe_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # ----------- 5. Heatmaps for Categorical Co-occurrence -----------\n",
    "\n",
    "categorical_pairs = [\n",
    "    (\"Gender\", \"Policy Type\"),\n",
    "    (\"Education Level\", \"Occupation\"),\n",
    "    (\"Location\", \"Property Type\"),\n",
    "    (\"Customer Feedback\", \"Smoking Status\")\n",
    "]\n",
    "\n",
    "for row_col, col_col in categorical_pairs:\n",
    "    cross_tab = pd.crosstab(df[row_col], df[col_col])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(cross_tab, annot=True, fmt=\"d\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "    plt.title(f\"Co-occurrence Heatmap: {row_col} vs {col_col}\")\n",
    "    plt.xlabel(col_col)\n",
    "    plt.ylabel(row_col)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    safe_name = f\"heatmap_{row_col}_{col_col}\".lower().replace(\" \", \"_\")\n",
    "    plt.savefig(f\"{output_dir}/{safe_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"✅ Multivariate plots saved to: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c675e99",
   "metadata": {},
   "source": [
    "## Identified correlations and trends that impact Premium Amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125676e0",
   "metadata": {},
   "source": [
    "# Premium vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e355dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"transformed.csv\")\n",
    "\n",
    "# === Output directory ===\n",
    "output_dir = \"output_graphs/bivariate\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Set plot style ===\n",
    "sns.set(style=\"whitegrid\")\n",
    "target_col = \"Premium Amount\"\n",
    "\n",
    "# === Identify column types ===\n",
    "numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Remove target from numerical features\n",
    "if target_col in numerical_cols:\n",
    "    numerical_cols.remove(target_col)\n",
    "\n",
    "# --- 1. Correlation heatmap ---\n",
    "corr = df[numerical_cols + [target_col]].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/correlation_heatmap.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 2. Boxplots: Categorical vs Target ---\n",
    "for col in categorical_cols:\n",
    "    if df[col].nunique() <= 10:  # Avoid high-cardinality plots\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(data=df, x=col, y=target_col)\n",
    "        plt.title(f\"{col} vs {target_col}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/boxplot_{col}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# --- 3. Scatterplots: Numerical vs Target ---\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(data=df, x=col, y=target_col, alpha=0.5)\n",
    "    plt.title(f\"{col} vs {target_col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/scatterplot_{col}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"✅ Bivariate EDA plots saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92773bb8",
   "metadata": {},
   "source": [
    "# Premium vs multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "TARGET = 'Premium Amount'\n",
    "INPUT_FILE = 'transformed.csv'\n",
    "OUTPUT_DIR = 'output_graphs/multivariate'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LOAD DATA\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"✅ Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# COLUMN TYPES\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop(TARGET, errors='ignore').tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "### 🔷 NUMERICAL FEATURES vs TARGET\n",
    "corr = df[numerical_cols + [TARGET]].corr()[[TARGET]].drop(TARGET)\n",
    "corr = corr.sort_values(by=TARGET, ascending=False)\n",
    "print(\"📊 Numerical correlations:\\n\", corr)\n",
    "\n",
    "for col in corr.index:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.regplot(data=df, x=col, y=TARGET, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n",
    "    plt.title(f'{col} vs {TARGET}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/{col}_vs_{TARGET}_regplot.png')\n",
    "    plt.close()\n",
    "\n",
    "### 🔶 CATEGORICAL FEATURES vs TARGET\n",
    "for col in categorical_cols:\n",
    "    if df[col].nunique() < 50:  # Skip too many-category columns\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.boxplot(data=df, x=col, y=TARGET)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'{col} vs {TARGET}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{OUTPUT_DIR}/{col}_vs_{TARGET}_boxplot.png')\n",
    "        plt.close()\n",
    "\n",
    "print(f\"✅ Multivariate plots saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b9a32",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"transformed.csv\")\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'], errors='coerce')\n",
    "df['Policy_Years_Since'] = datetime.now().year - df['Policy Start Date'].dt.year\n",
    "df.drop(columns=['Policy Start Date'], inplace=True)\n",
    "\n",
    "# Step 2: Separate target and features\n",
    "target = 'Premium Amount'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Step 3: Identify column types\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Step 4: Impute missing values\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X[numerical_cols] = num_imputer.fit_transform(X[numerical_cols])\n",
    "X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Step 5: Encode categoricals\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "encoded_cat = encoder.fit_transform(X[categorical_cols])\n",
    "encoded_cat_df = pd.DataFrame(encoded_cat, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Step 6: Scale numericals\n",
    "scaler = StandardScaler()\n",
    "scaled_num = scaler.fit_transform(X[numerical_cols])\n",
    "scaled_num_df = pd.DataFrame(scaled_num, columns=numerical_cols)\n",
    "\n",
    "# Step 7: Combine processed features\n",
    "X_processed = pd.concat([scaled_num_df.reset_index(drop=True), encoded_cat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Step 8: Random Forest for Feature Selection\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_processed, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_processed.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select top K features\n",
    "k = 30  # Change this as needed\n",
    "top_features = importance_df['Feature'].iloc[:k].tolist()\n",
    "X_selected_df = X_processed[top_features]\n",
    "\n",
    "# (Optional) Save selected data with target\n",
    "X_selected_df[target] = y.reset_index(drop=True)\n",
    "X_selected_df.to_csv(\"processed_rf_selected_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f73eb",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e77901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data = pd.read_csv(\"processed_rf_selected_data.csv\")\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(\"Premium Amount\", axis=1)\n",
    "y = data[\"Premium Amount\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save splits to disk for reuse\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42, verbosity=0)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    print(f\"{name} - MAE: {mae:.2f}, MSE: {mse:.2f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Save the best model manually\n",
    "    if name == \"Random Forest\":\n",
    "        joblib.dump(model, \"random_forest_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f298df",
   "metadata": {},
   "source": [
    "# Data Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ed6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved splits\n",
    "X_train = np.load(\"X_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
    "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
    "\n",
    "# --- Random Forest Hyperparameter Tuning ---\n",
    "print(\"\\n🔍 Tuning Random Forest...\")\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Best Random Forest Params:\", rf_grid.best_params_)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "# --- XGBoost Hyperparameter Tuning ---\n",
    "print(\"\\n🔍 Tuning XGBoost...\")\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [3, 6],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42, verbosity=0)\n",
    "xgb_grid = GridSearchCV(xgb, xgb_params, cv=3, scoring='r2', n_jobs=-1, verbose=1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Best XGBoost Params:\", xgb_grid.best_params_)\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\n📊 Final Evaluation on Test Set:\")\n",
    "for name, model in {\n",
    "    \"Random Forest (Tuned)\": rf_best,\n",
    "    \"XGBoost (Tuned)\": xgb_best\n",
    "}.items():\n",
    "    preds = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"MAE: {mae:.2f}, MSE: {mse:.2f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    filename = name.lower().replace(\" \", \"_\") + \".c\"\n",
    "    joblib.dump(model, filename)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
